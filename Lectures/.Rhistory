library(UsingR)
data(galton)
par(mfrow=c(1,2))
hist(galton$child,col="blue",breaks=100)
hist(galton$parent,col="blue",breaks=100)
plot(galton$parent,galton$child,pch=19,col="blue")
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
beta1
c(beta1, coef(lm(y ~ x))[2])
mean (yc)
mean (xc)
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
lm(yn ~ xn)
plot(galton$parent,galton$child,pch=19,col="blue")
abline(mean(y) - mean(x) * cor(y, x) * sd(y) / sd(x),
sd(y) / sd(x) * cor(y, x),
lwd = 3, col = "red")
abline(mean(y) - mean(x) * sd(y) / sd(x) / cor(y, x),
sd(y)/ cor(y, x) / sd(x),
lwd = 3, col = "blue")
abline(mean(y) - mean(x) * sd(y) / sd(x),
sd(y) / sd(x),
lwd = 2)
points(mean(x), mean(y), cex = 2, pch = 19)
data(diamond)
diamond
head(diamond)
plot(diamond$carat, diamond$price,
xlab = "Mass (carats)",
ylab = "Price (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(lm(price ~ carat, data = diamond), lwd = 2)
### 2. Linear regression
fit <- lm(price ~ carat, data = diamond)
coef(fit)
summary(fit)
### 3.  Regression to the mean
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
coef(fit2)
## The value of the slope is the same, however the value of the
## intercept changes
## Thus $500.1 is the expected price for the average sized diamond of the data (0.2042 carats).
## 4. Regression by rescaling the value of X
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
## 5. Predicting the price of the diamond by using a vector
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
## The easiest way to get the residuals
e <- resid(fit)
## Obtain the residuals manually, get the predicted Ys first
yhat <- predict(fit)
## The residuals are y - yhat.
residuals_manual <- y-yhat
## Let's check by comparing this with R's build in resid function
max(abs(e -(y - yhat)))
## Let's do it again hard coding the calculation of Yhat
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
g = ggplot(data.frame(x = x, y = resid(lm(y ~ x))),
aes(x = x, y = y))
g = g + geom_hline(yintercept = 0, size = 2)
g = g + geom_point(size = 7, alpha = .4, colour = "black")
g = g + geom_point(size = 5, alpha = .4, colour = "blue")
g = g + xlab("x") + ylab("Residual")
g
## Estimating the variance
data (diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
summary(fit)
sqrt(sum(resid(fit)^2) / (n - 2))
library(UsingR)
data(diamond)
y <- diamond$price
x <- diamond$carat
n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2))
ssx <- sum((x - mean(x))^2)
cor(y,x)
sd(y)
varBeta0 <- (1 / n + mean(x) ^ 2 / ssx) * sigma^2
varBeta1 <- sigma^2/ssx
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1
## Calculating the p-values
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
fit <- lm(y ~ x);
summary(fit)$coefficients
library(ggplot2)
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
###Multivariate Regressions
require(datasets); data(swiss); ?swiss
install.packages("GGally")
library(datasets); data(swiss); require(stats); require(graphics)
pairs(swiss, panel = panel.smooth, main = "Swiss data", col = 3 + (swiss$Catholic > 50))
summary(lm(Fertility ~ . , data = swiss))
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
##Dummy variables
require(datasets);data(InsectSprays); require(stats); require(ggplot2)
summary(lm(count ~ spray, data = InsectSprays))$coef
## Hard coding the dummy variable
summary(lm(count ~
I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
I(1 * (spray == 'F'))
, data = InsectSprays))$coef
summary(lm(count ~
I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +
I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
I(1 * (spray == 'F')) + I(1 * (spray == 'A')), data = InsectSprays))$coef
## What if we omit the intercept?
summary(lm(count ~ spray - 1, data = InsectSprays))$coef
unique(ave(InsectSprays$count, InsectSprays$spray))
## Reordering levels
spray2 <- relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))$coef
#Model selection
fit1<-lm(Fertility~Agriculture,data=swiss)
fit3 <- update(fit1, Fertility ~ Agriculture + Examination + Education, data=swiss)
fit5 <- update(fit1, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, data=swiss)
anova(fit1, fit3, fit5)
lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef
load("ravensData.rda")
setwd("/Users/diegorodrigues/Desktop/IP2022/IP2022/Lectures")
load("ravensData.rda")
head(ravensData)
lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef
lmRavens$coef[2]
lmRavens$coef[2]*63
x = seq(-10, 10, length = 1000)
beta0 = 0; beta1s = seq(.25, 1.5, by = .1)
plot(c(-10, 10), c(0, 1), type = "n", xlab = "X", ylab = "Probability", frame = FALSE)
sapply(beta1s, function(beta1) {
y = 1 / (1 + exp( -1 * ( beta0 + beta1 * x ) ))
lines(x, y, type = "l", lwd = 3)
}
)
## Consider setting b1 and varying b0
x = seq(-10, 10, length = 1000)
beta0s = seq(-2, 2, by = .5); beta1 = 1
plot(c(-10, 10), c(0, 1), type = "n", xlab = "X", ylab = "Probability", frame = FALSE)
sapply(beta0s, function(beta0) {
y = 1 / (1 + exp( -1 * ( beta0 + beta1 * x ) ))
lines(x, y, type = "l", lwd = 3)
}
)
x = seq(-10, 10, length = 1000)
beta0 = 0; beta1 = 1
p = 1 / (1 + exp(-1 * (beta0 + beta1 * x)))
y = rbinom(prob = p, size = 1, n = length(p))
plot(x, y, frame = FALSE, xlab = "x", ylab = "y")
lines(lowess(x, y), type = "l", col = "blue", lwd = 3)
fit = glm(y ~ x, family = binomial)
lines(x, predict(fit, type = "response"), lwd = 3, col = "red")
## Ravens Logistic Functions
logRegRavens = glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
## plotting the fit
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")
## to interpret the coefficientes let's exponentiate them
exp(logRegRavens$coeff)
exp(confint(logRegRavens))
anova(logRegRavens,test="Chisq")
read.dta("grogger.dta")
load(grogger.dta)
load("grogger.dta")
require(foreign)
installed.packages("foreign")
installed.packages(foreign)
install.packages("devtools")
require(foreign)
grogger <- read.dta("grogger.dta")
View(grogger)
lm1 = lm(grogger$avgsen ~ grogger$durat)
lm1
round(exp(coef(lm(I(log(grogger$avgsen + 1)) ~ grogger$durat))), 5)
glm1 = glm(grogger$avgsen ~ grogger$durat,family="poisson")
abline(lm1,col="red",lwd=3); lines(grogger$durat,glm1$fitted,col="blue",lwd=3)
glm1
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")
glm2 = glm(grogger$avgsen ~ grogger$durat,family="quasipoisson", data = grogger)
glm2
library(UsingR)
data(mtcars)
y = mtcars$mpg
x = cbind(1, mtcars$hp, mtcars$wt)
solve(t(x) %*% x) %*% t(x) %*% y
y = mtcars$mpg
x = cbind(1, mtcars$hp, mtcars$wt)
solve(t(x) %*% x) %*% t(x) %*% y
coef(lm(y~mtcars$hp+mtcars$wt,data=mtcars))
## Centering the data and taking the averages
n = nrow(x)
I = diag(rep(1, n))
H = matrix(1, n, n) / n
xt = (I - H) %*% x
apply(xt, 2, mean)
## Doing it using sweep
xt2 = sweep(x, 2, apply(x, 2, mean))
apply(xt2, 2, mean)
j <- cbind(rep(1,n))
xt = (I - j %*% solve(t(j) %*% j) %*% t(j)) %*% x
apply(xt, 2, mean)
## xt is the centered data of X
## yhat - y =x(x?x)^(-1)x?y
yhat <- x %*% solve(t(x) %*% x) %*% t(x) %*% y
## The residual "maker", where e = My
e <- (I - x %*% solve(t(x) %*% x) %*% t(x) ) %*% y
residuals(lm(y~mtcars$hp+mtcars$wt,data=mtcars))
